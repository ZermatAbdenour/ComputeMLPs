{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHOt6o8Mj3Ek",
        "outputId": "0ef6ffef-ecc7-4d99-89a7-e6a2ee9776d1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train = x_train.reshape(x_train.shape[0], -1).T.astype('f4')\n",
        "x_test = x_test.reshape(x_test.shape[0], -1).T.astype('f4')\n",
        "\n",
        "y_train, y_test = y_train.astype(np.int32), y_test.astype(np.int32)\n",
        "#train on subsets\n",
        "x_train = x_train[:,:10000]\n",
        "y_train = y_train[:10000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3MMseJVFuUb",
        "outputId": "e25ab6d9-aba1-48a9-f297-39e63e13ad6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(784, 10000)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yJ7NveDoDJ4X"
      },
      "outputs": [],
      "source": [
        "def init_params():\n",
        "    w1 = np.random.randn(10, 28*28).astype('f4') * np.sqrt(1. / (28*28))\n",
        "    b1 = np.random.randn(10, 1).astype('f4') * 0.01\n",
        "    w2 = np.random.randn(10, 10).astype('f4') * np.sqrt(1. / 10)\n",
        "    b2 = np.random.randn(10, 1).astype('f4') * 0.01\n",
        "    return w1, b1, w2, b2\n",
        "def ReLu(z):\n",
        "    return np.maximum(z, 0)\n",
        "\n",
        "def ReLu_deriv(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def Softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "def forward_prop(w1, b1, w2, b2, X):\n",
        "    z1 = np.dot(w1, X) + b1\n",
        "    a1 = ReLu(z1)\n",
        "    z2 = np.dot(w2, a1) + b2\n",
        "    a2 = Softmax(z2)\n",
        "    return z1, a1, z2, a2\n",
        "\n",
        "def one_hot(y, num_classes=10):\n",
        "    y = y.reshape(-1)\n",
        "    one_hot = np.zeros((num_classes, y.size))\n",
        "    one_hot[y, np.arange(y.size)] = 1\n",
        "    return one_hot\n",
        "\n",
        "def back_prop(z1, a1, z2, a2, w2, x, y):\n",
        "    m = y.size\n",
        "    one_hot_y = one_hot(y, num_classes=10)\n",
        "\n",
        "    dz2 = a2 - one_hot_y\n",
        "    dw2 = (1 / m) * np.dot(dz2, a1.T)\n",
        "    db2 = (1 / m) * np.sum(dz2, axis=1, keepdims=True)\n",
        "\n",
        "    dz1 = np.dot(w2.T, dz2) * ReLu_deriv(z1)\n",
        "    dw1 = (1 / m) * np.dot(dz1, x.T)\n",
        "    db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True)\n",
        "\n",
        "    return dw1, db1, dw2, db2\n",
        "\n",
        "def update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha):\n",
        "    w1 -= alpha * dw1\n",
        "    b1 -= alpha * db1\n",
        "    w2 -= alpha * dw2\n",
        "    b2 -= alpha * db2\n",
        "    return w1, b1, w2, b2\n",
        "\n",
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, axis=0)\n",
        "\n",
        "def get_accuracy(predictions, y):\n",
        "    return np.mean(predictions == y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkW38330FOIN",
        "outputId": "fa12c298-94ac-483b-b275-c928cc3d8af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0: Accuracy = 0.0702\n",
            "Iteration 50: Accuracy = 0.6583\n",
            "Iteration 100: Accuracy = 0.8243\n",
            "Iteration 150: Accuracy = 0.8614\n",
            "Iteration 200: Accuracy = 0.8787\n"
          ]
        }
      ],
      "source": [
        "def gradient_descent(X, Y, iterations, alpha):\n",
        "    w1, b1, w2, b2 = init_params()\n",
        "\n",
        "    for i in range(iterations):\n",
        "        z1, a1, z2, a2 = forward_prop(w1, b1, w2, b2, X)\n",
        "        dw1, db1, dw2, db2 = back_prop(z1, a1, z2, a2, w2, X, Y)\n",
        "        w1, b1, w2, b2 = update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha)\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            accuracy = get_accuracy(get_predictions(a2), Y)\n",
        "            print(f\"Iteration {i}: Accuracy = {accuracy:.4f}\")\n",
        "    return w1, b1, w2, b2\n",
        "\n",
        "w1, b1, w2, b2 = gradient_descent(x_train, y_train, iterations=250, alpha=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddnCimpAGhDN",
        "outputId": "b45a57bc-96e7-4498-b294-de9a183db051"
      },
      "outputs": [],
      "source": [
        "#test data\n",
        "def test_predictions(w1, b1, w2, b2,x,y):\n",
        "  _, _, _, a2 = forward_prop(w1, b1, w2, b2, x)\n",
        "  accuracy = get_accuracy(get_predictions(a2), y)\n",
        "def predict_single(w1, b1, w2, b2,x):\n",
        "  _, a1, _, a2 = forward_prop(w1, b1, w2, b2, x)\n",
        "  return a1,a2,get_predictions(a2)\n",
        "\n",
        "test_predictions(w1, b1, w2, b2,x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will start converting the nural network to work on the GPU using compute shaders we will start first by the forward pass \n",
        "we initialize our modernGL context and set some variables.\n",
        "N (Batch Size): Number of input samples processed at once.\n",
        "d (Input Dimension): Size of each input vector (784 for MNIST images of 28×28).\n",
        "h (Hidden Layer Size): Number of neurons in the hidden layer.\n",
        "num_classes (Output Size): Number of output classes (10 for MNIST digits 0-9)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import moderngl\n",
        "import numpy as np\n",
        "\n",
        "# Initialize ModernGL context\n",
        "ctx = moderngl.create_standalone_context()\n",
        "\n",
        "# Network dimensions\n",
        "INPUT_SIZE = 784\n",
        "HIDDEN_SIZE = 10\n",
        "OUTPUT_SIZE = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "[5]\n"
          ]
        }
      ],
      "source": [
        "DATA_POINT = 0\n",
        "\n",
        "\n",
        "#we transpose it becaue GPU require column-major order for matrices\n",
        "w1_buffer = ctx.buffer(w1.T.flatten().astype(np.float32).tobytes())\n",
        "b1_buffer = ctx.buffer(b1.flatten().astype(np.float32).tobytes())\n",
        "w2_buffer = ctx.buffer(w2.T.flatten().astype(np.float32).tobytes())\n",
        "b2_buffer = ctx.buffer(b2.flatten().astype(np.float32).tobytes())\n",
        "output_buffer = ctx.buffer(reserve=OUTPUT_SIZE * 4)\n",
        "\n",
        "a1,a2,pred = predict_single(w1, b1, w2, b2,x_train[:,DATA_POINT:DATA_POINT+1])\n",
        "print(y_train[DATA_POINT])\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "program = ctx.compute_shader(open(\"ForwardPass.glsl\").read())\n",
        "\n",
        "w1_buffer.bind_to_storage_buffer(1)\n",
        "b1_buffer.bind_to_storage_buffer(2)\n",
        "w2_buffer.bind_to_storage_buffer(3)\n",
        "b2_buffer.bind_to_storage_buffer(4)\n",
        "output_buffer.bind_to_storage_buffer(5)\n",
        "\n",
        "input_buffer = ctx.buffer(x_train[:,DATA_POINT].astype(np.float32).tobytes())\n",
        "input_buffer.bind_to_storage_buffer(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def predict_single_GPU():\n",
        "    # Run shader\n",
        "    program.run(1)\n",
        "    results = np.frombuffer(output_buffer.read(), dtype=np.float32)\n",
        "    # Get results\n",
        "    return np.argmax(results)\n",
        "\n",
        "#%timeit predict_single_GPU()\n",
        "#%timeit predict_single(w1, b1, w2, b2,x_train[:,DATA_POINT:DATA_POINT+1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "if we run the timeit benchmark for predicting a single data point in both the CPU and The GPU we found that the CPU is around 6 times faster than the GPU, CPU ~ 26.4 | GPU ~ 153. disapointing :(\n",
        "\n",
        "but if we take a closer look and separate the 2 main functions that are called in the GPU predict function we will find that each one of them take around 4 to 7 μs and 4+7 μs << 153 μs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%timeit program.run(1)\n",
        "#%timeit np.frombuffer(output_buffer.read(), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "this means that most of the time the CPU was waiting for the data to be transfered from the GPU to the CPU which is a slow process. so if we manage to minimize the numbers of GPU to CPU transfers we can boost the preformance significantly we cann't do that predicting single data point but we can do it when we dealing with large chunks or whole datasets using batches which GPU are really good at processing batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 6, 9, 7], dtype=int32)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and compile the compute shader\n",
        "batch_program = ctx.compute_shader(open(\"BatchForwardPass.glsl\").read())\n",
        "\n",
        "# Configuration\n",
        "INPUT_SIZE = 784\n",
        "HIDDEN_SIZE = 10\n",
        "OUTPUT_SIZE = 10\n",
        "\n",
        "num_samples = x_train.shape[1]\n",
        "output_data = np.zeros(num_samples * OUTPUT_SIZE, dtype=np.float32)\n",
        "output_buffer = ctx.buffer(output_data.tobytes())\n",
        "\n",
        "input_buffer.bind_to_storage_buffer(0)\n",
        "w1_buffer.bind_to_storage_buffer(1)\n",
        "b1_buffer.bind_to_storage_buffer(2)\n",
        "w2_buffer.bind_to_storage_buffer(3)\n",
        "b2_buffer.bind_to_storage_buffer(4)\n",
        "output_buffer.bind_to_storage_buffer(5)\n",
        "\n",
        "batch_program['num_inputs'] = INPUT_SIZE\n",
        "batch_program['output_size'] = OUTPUT_SIZE\n",
        "\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.32273716 -1.5838363  -2.6977243  ... -1.3151921  -0.03703864\n",
            "  -0.8281665 ]\n",
            " [ 8.206691   -4.0419908  -2.4474347  ... -2.2364566   0.917088\n",
            "  -2.7374642 ]\n",
            " [-2.6146932  -3.8923042   1.1048899  ... -2.237088   -0.16399984\n",
            "   1.4468384 ]\n",
            " ...\n",
            " [-2.6560068   1.4564618   2.3694801  ... -5.3120937   1.0176318\n",
            "  -2.6956584 ]\n",
            " [-4.229907   -4.8068438  -1.3575495  ...  0.81583834  0.41769403\n",
            "   3.4732785 ]\n",
            " [-2.0137994  -2.6409445  -3.023377   ...  6.88357    -0.8508768\n",
            "   4.452903  ]]\n",
            "Predictions: [2.9384259e-02 8.3258096e-03 2.7331968e-03 3.8146865e-01 1.3420635e-04\n",
            " 5.0992602e-01 3.0879001e-04 1.0891733e-02 3.9101381e-02 1.7725954e-02]\n"
          ]
        }
      ],
      "source": [
        "batch_program = ctx.compute_shader(open(\"FullForwardPass.glsl\").read())\n",
        "# Configuration\n",
        "INPUT_SIZE = 784\n",
        "HIDDEN_SIZE = 10\n",
        "OUTPUT_SIZE = 10\n",
        "\n",
        "num_samples = x_train.shape[1]\n",
        "\n",
        "input_buffer = ctx.buffer(x_train.T.flatten().astype(np.float32).tobytes())\n",
        "\n",
        "output_data = np.zeros(num_samples * OUTPUT_SIZE, dtype=np.float32)\n",
        "output_buffer = ctx.buffer(output_data.tobytes())\n",
        "\n",
        "input_buffer.bind_to_storage_buffer(0)\n",
        "w1_buffer.bind_to_storage_buffer(1)\n",
        "b1_buffer.bind_to_storage_buffer(2)\n",
        "w2_buffer.bind_to_storage_buffer(3)\n",
        "b2_buffer.bind_to_storage_buffer(4)\n",
        "output_buffer.bind_to_storage_buffer(5)\n",
        "batch_program['num_inputs'] = INPUT_SIZE\n",
        "batch_program['output_size'] = OUTPUT_SIZE\n",
        "\n",
        "batch_program.run(group_x=1, group_y=num_samples, group_z=1)\n",
        "# Read back results (if needed)\n",
        "output_buffer.read_into(output_data)\n",
        "reshaped_outputs = output_data.reshape(num_samples, OUTPUT_SIZE)\n",
        "\n",
        "def Softmax_GPU_output(x):\n",
        "    x_max = np.max(x, axis=1, keepdims=True)\n",
        "    e_x = np.exp(x - x_max)\n",
        "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "\n",
        "softmaxed_output = Softmax_GPU_output(reshaped_outputs)\n",
        "\n",
        "print(\"Predictions:\", softmaxed_output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
